{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce a Llama stack system awareness profiler method called `check_system_resources()`  in the llama_stack_client. Use `psutil` and `subprocess` to gather current CPU, Memory, GPU and Disk utilization, as the user instantiates the `LlamaStackClient`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"http://localhost\"\n",
    "PORT = 5001\n",
    "MODEL_NAME='Llama-3.2:latest'\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "client = LlamaStackClient(base_url=f\"{HOST}:{PORT}\")\n",
    "system_parameters = client.check_system_resources(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import groq\n",
    "from pathlib import Path\n",
    "\n",
    "groq_client = groq.Client(api_key=os.getenv(\"GROQ_KEY\"))\n",
    "completion = groq_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": Path('system_prompt.txt').read_text()\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": system_parameters}\n",
    "    ],\n",
    "    model=\"llama-3.1-70b-versatile\"\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "client.check_recommended_resources(json.loads(response))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
